{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:07:16.300209Z",
     "start_time": "2020-09-26T09:07:16.126647Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "import numpy as np\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix,make_scorer, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score,classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_path='Data/'\n",
    "\n",
    "import json\n",
    "with open(parent_path+'fear_speech_data.json', encoding = 'utf-8') as fp:\n",
    "    fear_speech_data=json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_text': '*рдкреНрд░рд╢рд╛рд╕рдХ рд╕рдорд┐рддрд┐*тЬКЁЯЪй  тЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧП тЧП тЧП тЧП ЁЯШОЁЯЪй *рдЖрдВрддрдХрд╡рд╛рджреА рд╕рдВрдЧрдардиреЛрдВ рдХрд╛ЁЯТгЁЯФк рдЗрд╕реНрд▓рд╛рдорд┐рдХ рдирд╛рдо рдФрд░ рдЙрдирдХрд╛ рдЗрд╕реНрд▓рд╛рдо.....* *рдзрд░реНрдо рд╕реЗ рдЬреБреЬрд╛ рд╣реБрдЖ рдЕрд░реНрде...* ЁЯРЦЁЯРЦЁЯРЦ *1.рд▓рд╢реНрдХрд░реЗ рддреИрдпрдмрд╛-рдлрд░рд┐рд╢реНрддреЛ рдХреА рд╕реЗрдирд╛* *2.рдЕрд▓ рдХрд╛рдпрджрд╛-рдЕрд▓реНрд▓рд╛рд╣ рдХрд╛ рдХрд╛рдпрджрд╛...* *3.рдЬреЗрд╢ рдП рдореЛрд╣рдореНрдж-рдореЛрд╣рдореНрдордж рд╕рд╛рд╣реЗрдм рдХрд╛ рджрд▓...* *4.рддрд╣рд░рд┐рдХ рдП рддрд╛рд▓рд┐рдмрд╛рди-рдкрд╡рд┐рддреНрд░ рдпреЛрджреНрдзрд╛рдУ рдХрд╛ рджрд▓...* *5.рд╣рд┐рдЬрдмреБрд▓ рдореБрдЬрд╛рджрд┐рди-рдЗрд╕реНрд▓рд╛рдореА рдмрд▓рд┐рджрд╛рдирд┐рдпреЛ рдХрд╛ рд╕рдореВрд╣...* *6.рдмреЛрдХреЛ рд╣рд░рд╛рдо -рдкреИрдЧрдореНрдмрд░ рдореБрд╣рдореНрдордж рдХреА рд╢рд┐рдХреНрд╖рд╛ рдХреЛ рдлреИрд▓рд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдкреНрд░рддрд┐рдмрджреНрдз..* *рд╕рднреА рдореБрд╕реНрд▓рдорд╛рди рдЕрдЬреНрдЮрд╛рдирд┐ рдЕрд▓реНрд▓рд╛рд╣ рдХреА рдмрддрд╛рдИ рд╣реБрдИ рд░реВрд╣рд╛рдиреА рдХрд┐рддрд╛рдм реШреБрд░рдЖрди рдХреА рдмрддрд╛рдИ рд░рд╛рд╣* *(рдкреВрд░реА рджреБрдирд┐рдпрд╛ рдХреЛ рдЗрд╕реНрд▓рд╛рдо рдмрдирд╛рдирд╛)рдкрд░ рд╣реА рдЪрд▓ рд░рд╣реЗ рд╣реИ рдХреЛрдИ рдЬреНрдпрд╛рджрд╛ рдмрдЪЁЯСи\\u200dЁЯСи\\u200dЁЯСж* *рдкреИрджрд╛ рдХрд░рдХреЗ рддреЛ рдХреЛрдИ рд▓рд╡ рдЬрд┐рд╣рд╛рджЁЯСл рдХрд░рдХреЗ рддреЛ рдХреЛрдИ рдХрд╛реЮрд┐рд░(рдЧреЗрд░ рдореБрд╕рд▓рдорд╛рди)рдХреЛ рдорд╛рд░рдХрд░..* *рдзрд░рддреА рдкрд░ рдЖрдВрддрдХЁЯФлЁЯТгЁЯТгЁЯТгЁЯФлрдлреЗрд▓рд╛ рд░рд╣реЗ рд╣реИ\\ufeffредред* ЁЯШбЁЯШбЁЯШбЁЯШОЁЯШбЁЯШбЁЯШб  *рдЬрдп рд╕рдирд╛рддрди рдзрд░реНрдо рдХреА*ЁЯЪйЁЯЪйЁЯЪй  ЁЯЩПЁЯЪйЁЯЗоЁЯЗ│ЁЯФ▒ЁЯП╣ЁЯРЪЁЯХЙ',\n",
       " 'translated_text': '* Administrator тЬК ЁЯЪй   Committee * тЧП тЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧПтЧП ЁЯЪй  ЁЯШО  тЧП тЧП тЧП рдирд╛рдо ЁЯФк  ЁЯТг  * Islamic name of terrorist organizations and their meaning ЁЯРЦ  ЁЯРЦ  ЁЯРЦ  in IslamтАж .. * ReligionтАж * ... 1. * 1. Army of Lashkar-e-Taiba-Farishto * * 2. Al Qaeda-Qaeda of Allah ... * * 3. Team of Jesh-e-Mohammad-Mohammed Saheb ... * * 4. Tahrik-e-Taliban-Holy Warriors Party ... * * 5. Hijbul Mujadin-group of Islamic sacrifices ... * * 6. Boko Haram - Committed to spread the teachings of Prophet Muhammad .. * * All the Muslim ignorance told the Quran of Allah the ЁЯСи ЁЯСж   ЁЯСи  spiritual book The path ЁЯСл  is going on * (making the whole world Islam), by creating ЁЯФл  ЁЯТг  ЁЯТг  ЁЯТг  ЁЯФл ЁЯШб ЁЯШб ЁЯШб ЁЯШб  ЁЯШб   ЁЯШО   ЁЯШб    some ЁЯЪй ЁЯЪй  ЁЯЩП ЁЯЗо ЁЯФ▒ ЁЯРЪ ЁЯХЙ   ЁЯП╣   ЁЯЗ│   ЁЯЪй   ЁЯЪй   more children * * *, by creating some love jihad and by killing a kafir (Ger Muslim) .. * * Terror on Earth ЁЯТгЁЯТгЁЯТгЁЯФлFella is there. * ЁЯШбЁЯШбЁЯШбЁЯШОЁЯШбЁЯШбЁЯШб * Jai of Sanatan Dharma * ЁЯЪйЁЯЪйЁЯЪй ЁЯЩПЁЯЪйЁЯЗоЁЯЗ│ЁЯФ▒ЁЯП╣ЁЯРЪЁЯХЙ  ',\n",
       " 'annotation_list': ['Fear speech', 'Fear speech', 'Normal'],\n",
       " 'propagation': [{'group_id': 9087,\n",
       "   'user_id': 229869,\n",
       "   'timestamp': 1538130086000},\n",
       "  {'group_id': 7, 'user_id': 215, 'timestamp': 1550186113000}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear_speech_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\AppData\\Local\\Temp\\ipykernel_15216\\400990223.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for key in tqdm_notebook(fear_speech_data.keys(),total=len(fear_speech_data)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf196cdd3e040868b33509be468f875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.preprocess import *\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "list_sents = []\n",
    "list_labels=[]\n",
    "for key in tqdm_notebook(fear_speech_data.keys(),total=len(fear_speech_data)):\n",
    "    element = fear_speech_data[key]\n",
    "    \n",
    "    count_fearspeech=element['annotation_list'].count('Fear speech')\n",
    "    count_normal=element['annotation_list'].count('Normal')\n",
    "    \n",
    "    if(count_fearspeech>count_normal):\n",
    "        one_fear_speech=1\n",
    "    else:\n",
    "        one_fear_speech=0\n",
    "    \n",
    "    text=preprocess_sent(element['message_text'],params={'remove_numbers': True, 'remove_emoji': True, 'remove_stop_words': False, 'tokenize': True})\n",
    "    list_sents.append(text)\n",
    "    list_labels.append(one_fear_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = np.array(list_sents,dtype='object')\n",
    "y_0 = np.array(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(model_name='lr'):\n",
    "    acc=[]\n",
    "    macro_f1=[]\n",
    "    prec=[]\n",
    "    recall=[]\n",
    "    prob=[]\n",
    "    auc_roc=[]\n",
    "    list_total_preds=[]\n",
    "    list_total_truth=[]\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle = True , random_state= 2020)\n",
    "\n",
    "    for train_index, test_index in skf.split(X_0, y_0):\n",
    "        print(\"TRAIN:\", train_index[0:5], \"TEST:\", test_index[0:5])\n",
    "        X_train, X_test = X_0[train_index], X_0[test_index]\n",
    "        y_train, y_test = y_0[train_index], y_0[test_index]\n",
    "\n",
    "        class_weights = dict(zip(np.unique(y_train), (np.sum(y_train.shape) / (len(np.unique(y_train)) * np.bincount(y_train)))))\n",
    "\n",
    "\n",
    "        print(class_weights)\n",
    "        ### Generate doc2vec vectors\n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train)]\n",
    "        model = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=10)\n",
    "        X_train_embed = np.array([list(model.infer_vector(ele)) for ele in X_train])\n",
    "        X_test_embed = np.array([list(model.infer_vector(ele)) for ele in X_test])\n",
    "        \n",
    "        if(model_name=='lr'):\n",
    "            classifier= LogisticRegression(class_weight='balanced',max_iter=500)\n",
    "        \n",
    "        elif(model_name=='svc'):\n",
    "            classifier=SVC(class_weight='balanced',kernel='rbf',probability=True)\n",
    "        \n",
    "        classifier.fit(X_train_embed, y_train)\n",
    "        y_pred=classifier.predict(X_test_embed)\n",
    "        y_pred_proba = classifier.predict_proba(X_test_embed)\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "        macro_f1.append(f1_score(y_test, y_pred, average='macro'))\n",
    "        auc_roc.append(roc_auc_score(y_test, y_pred_proba[:,1],average='macro'))\n",
    "        prec.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        prob.append(classifier.predict_proba(X_test_embed))\n",
    "        list_total_preds+=list(y_pred)\n",
    "        list_total_truth+=list(y_test)\n",
    "    return acc, macro_f1, prec, prob,auc_roc,list_total_preds,list_total_truth,prec,recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1 2 3 4] TEST: [ 6 12 13 17 21]\n",
      "{0: 0.6567651098901099, 1: 2.0947426067907995}\n",
      "TRAIN: [0 1 2 3 4] TEST: [11 14 15 20 26]\n",
      "{0: 0.6567651098901099, 1: 2.0947426067907995}\n",
      "TRAIN: [0 2 3 4 5] TEST: [ 1 18 19 24 28]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "TRAIN: [0 1 2 3 4] TEST: [ 7  8 23 29 32]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "TRAIN: [ 1  6  7  8 11] TEST: [0 2 3 4 5]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "Accuracy: 0.79 (+/- 0.01)\n",
      "Macro F1: 0.71 (+/- 0.02)\n",
      "Auc Roc F1: 0.79 (+/- 0.03)\n",
      "Precision for +ve class: 0.56 (+/- 0.03)\n",
      "Recall for +ve class: 0.58 (+/- 0.07)\n",
      "             precision    recall  f1-score  support  accuracy\n",
      "0             0.867355  0.853297  0.860269   3640.0  0.853297\n",
      "1             0.555371  0.584063  0.569356   1142.0  0.584063\n",
      "avg / total   0.711363  0.718680  0.714812   4782.0  0.789000\n"
     ]
    }
   ],
   "source": [
    "acc, macro_f1, prec, prob,auc_roc,list_total_preds,list_total_truth,prec,recall=model_run(model_name='svc')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(acc), np.std(acc) * 2))\n",
    "print(\"Macro F1: %0.2f (+/- %0.2f)\" % (np.mean(macro_f1), np.std(macro_f1) * 2))\n",
    "print(\"Auc Roc F1: %0.2f (+/- %0.2f)\" % (np.mean(auc_roc), np.std(auc_roc) * 2))\n",
    "print(\"Precision for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(prec), np.std(prec) * 2))\n",
    "print(\"Recall for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(recall), np.std(recall) * 2))\n",
    "print(pandas_classification_report(list_total_truth, list_total_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1 2 3 4] TEST: [ 6 12 13 17 21]\n",
      "{0: 0.6567651098901099, 1: 2.0947426067907995}\n",
      "TRAIN: [0 1 2 3 4] TEST: [11 14 15 20 26]\n",
      "{0: 0.6567651098901099, 1: 2.0947426067907995}\n",
      "TRAIN: [0 2 3 4 5] TEST: [ 1 18 19 24 28]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "TRAIN: [0 1 2 3 4] TEST: [ 7  8 23 29 32]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "TRAIN: [ 1  6  7  8 11] TEST: [0 2 3 4 5]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "Accuracy: 0.74 (+/- 0.02)\n",
      "Macro F1: 0.68 (+/- 0.03)\n",
      "Auc Roc: 0.77 (+/- 0.02)\n",
      "Precision for +ve class: 0.47 (+/- 0.04)\n",
      "Recall for +ve class: 0.64 (+/- 0.08)\n",
      "             precision    recall  f1-score  support  accuracy\n",
      "0             0.872406  0.773901  0.820207   3640.0  0.773901\n",
      "1             0.470058  0.639229  0.541744   1142.0  0.639229\n",
      "avg / total   0.671232  0.706565  0.680975   4782.0  0.741740\n"
     ]
    }
   ],
   "source": [
    "acc, macro_f1, prec, prob,auc_roc,list_total_preds,list_total_truth,prec,recall=model_run(model_name='lr')\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(acc), np.std(acc) * 2))\n",
    "print(\"Macro F1: %0.2f (+/- %0.2f)\" % (np.mean(macro_f1), np.std(macro_f1) * 2))\n",
    "print(\"Auc Roc: %0.2f (+/- %0.2f)\" % (np.mean(auc_roc), np.std(auc_roc) * 2))\n",
    "print(\"Precision for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(prec), np.std(prec) * 2))\n",
    "print(\"Recall for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(recall), np.std(recall) * 2))\n",
    "print(pandas_classification_report(list_total_truth, list_total_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
